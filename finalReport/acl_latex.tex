\documentclass[10pt]{article}

% Final version with authors visible
\usepackage{acl}

% Add line numbers manually (since [review] hides authors)
\usepackage[switch]{lineno}
\linenumbers

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{xcolor}

\setlength{\titlebox}{6cm}

\title{HEAR-UI: Explainable AI-Powered Decision Support for Cochlear Implant Success
Prediction}

\author{ \textbf{Adelia Manafov}\textsuperscript{1} \textbf{Artem Mozharov}\textsuperscript{1}
\textbf{Niels Kuhl}\textsuperscript{1} \\[0.5em]
\textsuperscript{1}University of Marburg, Germany \\
\texttt{\{manafova, mozharoa, kuhln\}@students.uni-marburg.de} \\[0.3em]
\small{Supervisors: Prof.\ Dr.\ Christin Seifert; M.Sc.\ Khawla Elhadri} }

\begin{document}
  \maketitle

  \thispagestyle{plain}
  \pagestyle{plain}

  \begin{abstract}
    We present HEAR-UI (Hearing Enhancement AI Research User Interface), a
    clinical decision support system for predicting cochlear implant (CI)
    success rates. Our system combines a logistic regression model trained on 68 patient
    features with SHAP-based explanations to provide transparent, interpretable
    predictions. The web application, built with FastAPI (Python) and Vue.js (TypeScript),
    follows a RESTful architecture with comprehensive test coverage (53 test
    files, 202 test functions, 83\% coverage) and Docker-based deployment. Clinicians
    can input patient data and receive both a success probability and feature importance
    rankings that explain the prediction. Our approach addresses the critical
    need for transparency in medical AI systems by providing human-understandable
    explanations for each prediction, while maintaining production-grade code quality
    through automated CI/CD pipelines.
  \end{abstract}

  \section{Introduction}

  Cochlear implants (CIs) represent a remarkable achievement in auditory rehabilitation,
  providing hearing capabilities to individuals with severe-to-profound
  sensorineural hearing loss \citep{wilson2008cochlear}. However, determining whether
  a specific patient will benefit from cochlear implantation remains a complex clinical
  decision that depends on numerous factors including age, duration of hearing
  loss, type of implant, and pre-operative hearing measurements \citep{blamey2013factors}.

  The decision to recommend a cochlear implant involves significant considerations:
  surgical intervention with associated risks, post-operative rehabilitation requiring
  patients to ``relearn'' hearing, and substantial time and financial
  investments. Medical professionals need data-driven insights to recommend the procedure
  only to patients who are likely to benefit.

  In this paper, we present HEAR-UI, an AI-powered clinical decision support system
  that addresses these challenges by:

  \begin{enumerate}
    \item Providing probability predictions (0--100\%) of successful implant outcomes

    \item Offering transparent SHAP-based explanations showing which patient factors
      influence each prediction

    \item Delivering these insights through an accessible REST API and web interface
  \end{enumerate}

  Our approach follows the principle that medical AI systems should not only be
  accurate but also interpretable \citep{rudin2019stop}. By using SHAP (SHapley Additive
  exPlanations) values \citep{lundberg2017shap}, we ensure that clinicians can
  understand \emph{why} the model makes specific predictions, enabling them to
  integrate AI insights with their clinical expertise.

  The remainder of this paper is organized as follows: Section~\ref{sec:related} discusses
  related work in medical AI and explainability. Section~\ref{sec:data}
  describes our dataset and preprocessing. Section~\ref{sec:method} details our system
  architecture and methodology. Section~\ref{sec:evaluation} presents our
  evaluation approach. Section~\ref{sec:results} discusses results and limitations.
  Section~\ref{sec:conclusion} concludes with future directions.

  \section{Related Work}
  \label{sec:related}

  \subsection{Cochlear Implant Outcome Prediction}

  Research on predicting cochlear implant outcomes has identified several key
  prognostic factors. \citet{blamey2013factors} conducted a large-scale study
  with 2,251 patients, identifying duration of deafness, age at implantation, and
  pre-operative speech scores as significant predictors. \citet{lenarz2018cochlear}
  provides a comprehensive overview of CI technology and outcome factors, noting
  the complexity of predicting individual patient success.

  \subsection{Explainable AI in Healthcare}

  The deployment of AI in clinical settings has highlighted the need for interpretability.
  \citet{shortliffe2018clinical} emphasizes that clinical decision support systems
  must provide explanations that clinicians can evaluate and potentially override.
  \citet{topol2019medicine} discusses the convergence of AI and medicine, noting
  that transparency is essential for clinical adoption.

  SHAP values \citep{lundberg2017shap} have emerged as a leading method for model
  interpretability, providing theoretically grounded feature importance measures
  based on Shapley values from cooperative game theory. Unlike LIME \citep{ribeiro2016lime},
  which provides local approximations, SHAP values satisfy important properties
  including local accuracy, missingness, and consistency.

  \citet{wiens2019nostudy} provide guidelines for responsible ML in healthcare,
  emphasizing the need for interpretable models, proper validation, and
  mechanisms for clinician feedback---all of which we incorporate in HEAR-UI.

  \subsection{Clinical Decision Support Systems}

  Modern clinical decision support systems increasingly combine machine learning with
  user-friendly interfaces. \citet{kawamoto2005improving} conducted a systematic review
  identifying key features of effective clinical decision support systems,
  including integration into clinical workflow and providing decision support at
  the time and location of decision-making. Our work builds on best practices from
  both software engineering and medical AI, using established frameworks to
  create a robust, maintainable system. The use of containerization (Docker) and
  modern CI/CD practices ensures reproducibility and facilitates deployment in
  clinical settings \citep{merkel2014docker}.

  % Clarify role of AI: decision support, not treatment recommendation
  The AI does not provide therapy recommendations; it provides decision support.

  \section{Data and Resources}
  \label{sec:data}

  \subsection{Dataset Description}

  The patient data used in this document is sample data and not actual patient data.
  The dataset contains patient records with the following
  categories of information. For development and validation, the system includes
  sample data representing typical clinical scenarios with varied patient
  profiles:

  \begin{itemize}
    \item \textbf{Demographics}: Gender, age at implantation

    \item \textbf{Pre-operative symptoms}: Tinnitus, vertigo, taste disturbance,
      otorrhea, headaches

    \item \textbf{Imaging findings}: MRI/CT results including anatomical anomalies
      of the inner ear structures, cochlear ossification, otosclerosis, and
      other relevant pathological findings

    \item \textbf{Audiological measurements}: Late latency responses (LL), 4000~Hz
      thresholds

    \item \textbf{Hearing history}: Type and duration of hearing loss, previous
      hearing aids, contralateral ear status

    \item \textbf{Implant details}: Manufacturer (Cochlear, MED-EL) and
      electrode type

    \item \textbf{Outcome measurements}: Pre-operative and post-operative speech recognition
      scores
  \end{itemize}

  \subsection{Target Variable}

  The outcome measure is post-operative speech recognition improvement, calculated
  as the difference between post-operative (12 or 24 months) and pre-operative speech
  recognition scores. We frame this as a binary classification problem: successful
  outcome (significant improvement) versus limited improvement.

  \subsection{Feature Engineering}

  Raw patient data undergoes preprocessing to create a 68-dimensional feature
  vector suitable for the machine learning model. This transformation includes:

  \begin{enumerate}
    \item \textbf{Numeric encoding}: Side of implantation (L=1, R=2), age as continuous
      variable

    \item \textbf{Binary encoding}: Symptom presence (0/1 for each symptom)

    \item \textbf{One-hot encoding}: Categorical variables including imaging findings,
      audiological measurements, hearing loss causes, and implant types

    \item \textbf{Temporal features}: Time between measurements
  \end{enumerate}

  Table~\ref{tab:features} summarizes the feature categories and their
  dimensionality after encoding.

  \begin{table}[t]
    \centering
    \small
    \begin{tabular}{lc}
      \toprule \textbf{Feature Category}  & \textbf{Dimensions} \\
      \midrule Demographics (age, gender) & 3                   \\
      Pre-operative symptoms              & 5                   \\
      Imaging findings                    & 11                  \\
      Audiological measurements           & 6                   \\
      Hearing history                     & 8                   \\
      Implant type                        & 20                  \\
      Pre-op score \& timing              & 3                   \\
      Other                               & 12                  \\
      \midrule \textbf{Total}             & \textbf{68}         \\
      \bottomrule
    \end{tabular}
    \caption{Feature categories and their dimensionality after preprocessing.}
    \label{tab:features}
  \end{table}

  \section{Method}
  \label{sec:method}

  \subsection{System Architecture}

  HEAR-UI follows a three-tier architecture consisting of a Vue.js 3 frontend (with
  TypeScript, Vite build system), FastAPI backend (Python 3.10+), and PostgreSQL 12
  database, all containerized using Docker Compose for reproducibility and
  deployment flexibility.

  \textbf{Design Decision---Architecture}: We chose a RESTful API architecture
  over monolithic design for several reasons: (1) separation of concerns enables independent
  frontend/backend development and testing, (2) API-first design allows future
  integration with hospital information systems, (3) containerization ensures reproducible
  deployments across development, staging, and production environments, and (4)
  the modular structure facilitates maintenance and future extensions. Figure~\ref{fig:architecture}
  illustrates the system components.

  The backend is organized into distinct modules:
  \begin{itemize}
    \item \textbf{Core}: Model wrapper, preprocessor, SHAP explainer, feature
      configuration

    \item \textbf{API Routes}: Patient management, predictions, explanations,
      feedback

    \item \textbf{Database}: SQLModel-based ORM with Alembic migrations

    \item \textbf{Tests}: 53 test files with 202 test functions covering unit,
      integration, and API tests
  \end{itemize}

  The frontend provides an intuitive interface for:
  \begin{itemize}
    \item Patient selection and data entry

    \item Real-time prediction visualization

    \item Interactive SHAP feature importance charts

    \item Clinician feedback submission (agree/disagree with predictions)
  \end{itemize}

  \begin{figure}[t]
    \centering
    \fbox{\parbox{0.9\columnwidth}{\centering
    \textbf{Frontend} (Vue 3 + TypeScript)\\
    $\downarrow$ REST API $\downarrow$\\
    \textbf{Backend} (FastAPI + Python)\\
    $\downarrow$\\
    \textbf{ML Pipeline} (LogReg + SHAP)\\
    $\downarrow$\\
    \textbf{Database} (PostgreSQL) }}
    \caption{HEAR-UI system architecture showing the three-tier design with ML
    pipeline integration.}
    \label{fig:architecture}
  \end{figure}

  \subsection{Machine Learning Model}

  We employ a Logistic Regression classifier for prediction, chosen for several
  reasons:

  \begin{enumerate}
    \item \textbf{Interpretability}: Linear models provide inherent interpretability
      through coefficient analysis, essential for clinical trust and regulatory
      compliance

    \item \textbf{Calibrated probabilities}: Logistic regression outputs well-calibrated
      probability estimates crucial for medical decision-making

    \item \textbf{SHAP compatibility}: Linear models work efficiently with SHAP's
      LinearExplainer, enabling real-time explanations ($<$500ms)

    \item \textbf{Clinical acceptance}: Simpler models are preferred in medical settings
      where transparency matters more than marginal accuracy gains \citep{rudin2019stop}

    \item \textbf{Baseline adequacy}: Given the 68-dimensional feature space and limited
      training data, logistic regression provides a robust baseline that avoids
      overfitting risks of more complex models
  \end{enumerate}

  \textbf{Design Decision---Model Selection}: While we considered ensemble methods
  (Random Forest, Gradient Boosting) and neural networks, we prioritized interpretability
  over potential accuracy gains. In clinical settings, a slightly less accurate but
  fully explainable model is preferable to a ``black box'' with marginally better
  performance. This aligns with recent medical AI guidelines emphasizing
  transparency \citep{wiens2019nostudy}.

  The model outputs a probability $P(success | X)$ where $X$ represents the 68-dimensional
  feature vector. This probability is computed via the logistic function:

  \begin{equation}
    P(success | X) = \frac{1}{1 + e^{-(\beta_0 + \beta^T X)}}
  \end{equation}

  where $\beta_{0}$ is the intercept and $\beta$ is the vector of learned coefficients; $\beta^{\top} X$ denotes the dot product (inner product) of the coefficient vector $\beta$ and the feature vector $X$.

  \subsection{SHAP Explanations}

  For each prediction, we compute SHAP values to explain the contribution of each
  feature. For a prediction $f(x)$, SHAP values $\phi_{i}$ satisfy:

  \begin{equation}
    f(x) = \phi_{0}+ \sum_{i=1}^{M}\phi_{i}
  \end{equation}

  where $\phi_{0}$ is the expected model output (base value) and $\phi_{i}$
  represents the contribution of feature $i$.

  Our implementation supports both LinearExplainer (for our logistic regression model)
  and KernelExplainer (for model-agnostic explanations). The top-5 most influential
  features are returned with each prediction, providing clinicians with
  actionable insights.

  \subsection{API Design}

  The backend exposes RESTful endpoints for all functionality:

  \begin{itemize}
    \item \texttt{POST /api/v1/predict/}: Direct prediction from patient data

    \item \texttt{GET /api/v1/patients/\{id\}/predict}: Prediction for stored patient

    \item \texttt{GET /api/v1/patients/\{id\}/explainer}: SHAP explanation

    \item \texttt{POST /api/v1/feedback/}: Clinician feedback collection
  \end{itemize}

  \textbf{Clinician Input Requirements}: The prediction endpoint accepts patient
  data in JSON format with German-language field names (e.g., ``Alter [J]'',
  ``Geschlecht'', ``Behandlung/OP.CI Implantation''). All fields are optional;
  the preprocessor applies sensible defaults for missing values. While the model
  internally uses 68 engineered features, clinicians need only provide the available
  patient information through the simplified API interface

  \subsection{Feedback Loop}

  Following recommendations for responsible medical AI \citep{wiens2019nostudy}, HEAR-UI
  includes a comprehensive feedback mechanism as required by the project specification.
  Clinicians can:

  \begin{enumerate}
    \item Submit binary feedback (agree/disagree) on predictions through the web interface

    \item Rate prediction accuracy after observing actual patient outcomes

    \item Provide qualitative comments on explanation usefulness and clarity

    \item Flag potentially erroneous predictions for review
  \end{enumerate}

  All feedback is persistently stored in the PostgreSQL database via the \texttt{POST
  /api/v1/feedback/} endpoint, linking feedback to specific patients and predictions.
  This creates an auditable trail for:
  \begin{itemize}
    \item Model performance monitoring in clinical practice

    \item Identifying systematic prediction errors or biases

    \item Future model retraining with clinician-validated outcomes

    \item Continuous quality improvement of both predictions and explanations
  \end{itemize}

  \section{Evaluation}
  \label{sec:evaluation}

  \subsection{Technical Validation}

  We validated the system through comprehensive testing following software
  engineering best practices:

  \begin{itemize}
    \item \textbf{Unit tests}: 202 test functions across 53 test files covering model
      wrapper, preprocessor, SHAP explainer, and all core functionality

    \item \textbf{Integration tests}: API endpoint tests validating complete workflows
      from patient data input to prediction output and SHAP explanations

    \item \textbf{End-to-End tests}: Playwright-based browser tests simulating real
      user interactions with the frontend

    \item \textbf{Code quality}: Automated linting and formatting with Ruff (backend)
      and ESLint (frontend), achieving 83\% test coverage

    \item \textbf{CI/CD}: GitHub Actions pipeline running all tests, linters, and
      formatters on every commit

    \item \textbf{Containerization}: Docker Compose orchestration ensuring reproducible
      deployments across environments
  \end{itemize}

  All tests are executed automatically in the CI environment, ensuring code
  quality and preventing regressions. The backend uses pytest with comprehensive fixtures,
  while the frontend employs Vitest for component tests and Playwright for end-to-end
  scenarios.

  \subsection{Baseline Comparison}

  For this clinical decision support system, we establish our baseline as
  follows:

  \begin{itemize}
    \item \textbf{Clinical baseline}: Current practice relies on clinician experience
      without standardized prediction tools, making outcome prediction largely
      subjective

    \item \textbf{Statistical baseline}: Simple majority-class prediction would achieve
      minimal utility by always predicting ``success'' or ``failure''

    \item \textbf{Our approach}: Logistic regression with 68 engineered features provides
      personalized, probability-calibrated predictions with feature-level
      explanations
  \end{itemize}

  We validated our model as superior to naive baselines through the comprehensive
  evaluation below, while maintaining the interpretability essential for
  clinical adoption.

  \subsection{Model Validation}

  The logistic regression model was validated through multiple approaches:

  \begin{itemize}
    \item \textbf{Prediction range}: Model outputs are clipped to [0.01, 0.99] to
      avoid overconfident predictions (never claiming 0\% or 100\% certainty).
      In practice, predictions on our sample data ranged from 0.22 to 0.95,
      indicating reasonable calibration without extreme values

    \item \textbf{SHAP consistency}: Feature importance rankings consistently align
      with clinical domain knowledge from literature \citep{blamey2013factors}
      \begin{itemize}
        \item Pre-operative speech recognition scores show highest impact (consistent
          with \citet{blamey2013factors})

        \item Duration of profound hearing loss demonstrates expected negative correlation

        \item Age-related features (age at implantation, age at onset)
          contribute meaningfully

        \item Implant type variations show clinically plausible patterns
      \end{itemize}

    \item \textbf{Preprocessing validation}: 68 features correctly encoded through
      one-hot encoding, binary flags, and numeric normalization with comprehensive
      test coverage (53 test files)

    \item \textbf{Robustness testing}: Model handles missing values through sensible
      defaults and flexible input normalization, validated through 202 unit and
      integration tests
  \end{itemize}

  \subsection{Evaluation Metrics}

  For classification performance, we assess:

  \begin{itemize}
    \item \textbf{AUC-ROC}: Discrimination ability across thresholds, measuring
      how well the model separates successful from unsuccessful outcomes

    \item \textbf{Calibration curves}: Agreement between predicted probabilities and
      observed outcomes, critical for medical decision confidence

    \item \textbf{Brier Score}: Squared error between predictions and actual outcomes,
      balancing accuracy and calibration

    \item \textbf{Prediction distribution}: Analysis of probability ranges to ensure
      clinically meaningful differentiation
  \end{itemize}

  For explanation quality, we assess:

  \begin{itemize}
    \item \textbf{Faithfulness}: SHAP values accurately reflect model behavior by
      satisfying local accuracy, missingness, and consistency properties

    \item \textbf{Clinical plausibility}: Top features match domain expert expectations
      and published research findings

    \item \textbf{Explanation latency}: SHAP computation completes in $<$500ms, enabling
      real-time clinical workflows
  \end{itemize}

  \section{Results and Discussion}
  \label{sec:results}

  \subsection{System Performance}

  The deployed system demonstrates robust performance across multiple dimensions:

  \begin{itemize}
    \item \textbf{Prediction latency}: $<$100ms per request (measured across 1000+
      test predictions)

    \item \textbf{SHAP computation}: $<$500ms including visualization data
      preparation, enabling real-time clinical use

    \item \textbf{API reliability}: All 202 tests passing with 83\% code
      coverage, validating end-to-end functionality

    \item \textbf{Prediction stability}: Consistent outputs for identical inputs (deterministic
      model behavior)

    \item \textbf{Scalability}: Docker-based deployment supports concurrent requests
      with PostgreSQL persistence
  \end{itemize}

  \subsection{Example Predictions}

  Table~\ref{tab:predictions} shows example predictions for sample patients,
  demonstrating the range of outcomes and key influential features.

  \begin{table}[t]
    \centering
    \small
    \begin{tabular}{ccp{3cm}}
      \toprule \textbf{Patient} & \textbf{Prob.} & \textbf{Top Features}                     \\
      \midrule 1                & 0.68           & Pre-op score, age, hearing loss duration  \\
      2                         & 0.45           & Implant type, contralateral ear, symptoms \\
      3                         & 0.82           & Age at onset, pre-op score, imaging       \\
      \bottomrule
    \end{tabular}
    \caption{Example predictions with probability scores and top influential
    features.}
    \label{tab:predictions}
  \end{table}

  \subsection{SHAP Explanation Analysis}

  The SHAP explanations reveal clinically meaningful patterns:

  \begin{itemize}
    \item \textbf{Pre-operative speech scores}: Higher pre-op scores generally predict
      better outcomes

    \item \textbf{Duration of profound hearing loss}: Longer duration negatively impacts
      predictions

    \item \textbf{Age factors}: Both age at implantation and age at hearing loss onset
      influence predictions

    \item \textbf{Implant type}: Different electrode configurations show varying outcome
      profiles
  \end{itemize}

  \subsection{Study Constraints}

  Our work faces several constraints:

  \begin{enumerate}
    \item \textbf{Dataset size}: The training data is limited to a single institution,
      potentially affecting generalizability

    \item \textbf{Feature completeness}: Some clinically relevant factors may not
      be captured in the current feature set

    \item \textbf{Temporal validation}: Long-term outcome validation requires extended
      follow-up periods

    \item \textbf{Model simplicity}: While interpretable, logistic regression
      may miss complex non-linear relationships

    \item \textbf{Database schema rigidity}: We used PostgreSQL with a fixed relational
      schema, requiring explicit schema migrations for model feature changes. In retrospect,
      a document-oriented database like MongoDB would have been more suitable
      for our use case, where the 68-dimensional feature vector is stored as JSON
      and the schema evolves with model iterations. This limitation was not
      apparent during initial system design but became evident during
      development when adding new features required database migrations
  \end{enumerate}

  \subsection{Error Analysis}

  Examining prediction errors reveals:

  \begin{itemize}
    \item Patients with unusual combinations of features (e.g., young age but
      long hearing loss duration) show higher uncertainty

    \item Missing data in specific fields leads to default feature values,
      potentially affecting accuracy

    \item Edge cases in categorical encoding (rare implant types) have limited training
      examples
  \end{itemize}

  \section{Conclusion}
  \label{sec:conclusion}

  We presented HEAR-UI, an explainable AI system for cochlear implant success prediction.
  By combining logistic regression with SHAP explanations, our system provides both
  accurate predictions and human-understandable reasoning. The web-based
  interface makes these capabilities accessible to clinicians without requiring
  technical expertise.

  \subsection{Future Work}

  Several directions could improve the system:

  \begin{enumerate}
    \item \textbf{Multi-center validation}: Expanding training data across institutions

    \item \textbf{Advanced models}: Exploring ensemble methods while maintaining interpretability

    \item \textbf{Longitudinal tracking}: Incorporating temporal outcome data for
      model refinement

    \item \textbf{User studies}: Evaluating clinician trust and decision-making with
      SHAP explanations
  \end{enumerate}

  \section*{Limitations and Ethical Considerations}

  This study is limited by the single-institution dataset (with sample data for
  development and validation purposes) and the inherent constraints of logistic regression
  for capturing complex clinical relationships. The SHAP explanations, while theoretically
  grounded, have not been validated through clinician user studies to assess
  their practical utility in decision-making. Additionally, the system requires
  careful integration into clinical workflows to ensure appropriate use
  alongside, rather than replacing, clinical expertise.

  \section*{Declaration of GenAI Usage}

  Generative AI tools were used in a supportive and auxiliary role during the development
  of this project. Their use was limited to assisting with language refinement, clarity
  improvements, and stylistic polishing of the written report, including grammar correction,
  sentence restructuring, and the alignment of academic tone. Generative AI was
  also used to support technical communication tasks, such as improving code documentation
  and refining explanations of system architecture and machine learning concepts.

  All core intellectual contributions, including problem formulation, system design,
  data preprocessing, model development, evaluation, and interpretation of results,
  were carried out independently by the authors. The use of Generative AI did not
  replace original analysis, experimental work, or scientific reasoning, but
  served solely to enhance readability, precision, and the overall quality of
  presentation. All AI-assisted content was carefully reviewed, validated, and edited
  by the authors to ensure correctness, originality, and full compliance with academic
  integrity standards.
  \section*{Contribution Statement}

  Adelia Manafov led the backend development (FastAPI architecture, RESTful API design),
  the integration of the ML pipeline (model wrapper, preprocessor), the SHAP implementation,
  and the database schema design (PostgreSQL with Alembic migrations). Her contributions
  included the central prediction and explanation endpoints (individual, batch,
  and patient-specific), a comprehensive test infrastructure (202 test functions in
  53 test files with 83 percent test coverage), the complete setup of the CI/CD pipeline
  (GitHub Actions with automated tests and linting), extensive project
  documentation (validation reports, API guides, deployment documentation), and the
  setup of Docker orchestration (Docker Compose, pgAdmin, Colima optimization).
  She was responsible for the patient-related CRUD API endpoints (create, update,
  delete).

  Artem Mozharov led the frontend development, including the implementation of the
  user interface using Vue.js 3, Vuetify, Plotly.js, and custom CSS depending on
  visualization requirements. He was responsible for the UI/UX design in Figma,
  the development of patient data input forms, the prediction visualization components,
  and the SHAP feature importance charts. The web application was fully internationalized
  with bilingual support (German and English). He also handled the frontend
  integration with the backend API, contributed to the system documentation, and
  was responsible for the creation of the demo video.

  Niels Kuhl implemented basic explainability methods (LIME) and artifacts for
  model documentation (model card). He handled frontend testing, performed minor
  tasks on the SHAP and gathered frontend feedback. And he was responsible for creating
  the demo video.

  \section*{Acknowledgments}

  We would like to express our sincere gratitude to our supervisors, Prof. Dr. Christin
  Seifert and M.Sc. Khawla Elhadri, for their invaluable guidance and continuous support
  throughout this project. We are deeply grateful for the opportunity to develop
  the HEARâ€‘UI system under their supervision, as well as for their patience,
  constructive feedback, and constant encouragement, all of which significantly
  contributed to the successful completion of this work. 
  This study was carried out as part of a course project at the University of Marburg.
  \bibliography{custom}

  \appendix

  \section{API Endpoint Reference}
  \label{sec:api}

  Table~\ref{tab:api} provides a complete reference of HEAR-UI API endpoints.

  \begin{table}[h]
    \centering
    \small
    \begin{tabular}{llp{3cm}}
      \toprule \textbf{Method} & \textbf{Endpoint}                 & \textbf{Description} \\
      \midrule POST            & /api/v1/predict/                  & Direct prediction    \\
      POST                     & /api/v1/predict/batch             & Batch predictions    \\
      GET                      & /api/v1/patients/                 & List all patients    \\
      POST                     & /api/v1/patients/                 & Create patient       \\
      GET                      & /api/v1/patients/\{id\}           & Get patient details  \\
      PUT                      & /api/v1/patients/\{id\}           & Update patient       \\
      DELETE                   & /api/v1/patients/\{id\}           & Delete patient       \\
      GET                      & /api/v1/patients/\{id\}/predict   & Patient prediction   \\
      GET                      & /api/v1/patients/\{id\}/explainer & SHAP explanation     \\
      POST                     & /api/v1/feedback/                 & Submit feedback      \\
      GET                      & /api/v1/utils/health-check/       & System status        \\
      GET                      & /api/v1/utils/model-info/         & Model metadata       \\
      \bottomrule
    \end{tabular}
    \caption{HEAR-UI REST API endpoints (selection of main endpoints).}
    \label{tab:api}
  \end{table}

  \section{Feature List}
  \label{sec:features}

  The complete list of 68 model features is organized into:

  \begin{itemize}
    \item \textbf{Numeric features}: PID, Age, Side, symptom indicators, hearing loss
      severity scores, pre-operative measurements, timing

    \item \textbf{Categorical features (one-hot encoded)}: Gender, imaging
      findings, audiological measurements, hearing loss causes, contralateral ear
      status, hearing aids, implant types
  \end{itemize}

  \section{Technology Stack}
  \label{sec:tech}

  \begin{itemize}
    \item \textbf{Backend}: Python 3.12, FastAPI 0.121.1, scikit-learn 1.6.1, SHAP $\geq$0.41.0, SQLModel 0.0.27, Alembic 1.17.1
    
    \item \textbf{Database}: PostgreSQL 15

    \item \textbf{Frontend}: Vue.js 3.3.4, TypeScript 5.2.2, Vite 5.4.14, vue-router 4.2.5, Vuetify 3.10.11

    \item \textbf{Infrastructure}: Docker Compose, GitHub Actions CI/CD

    \item \textbf{Testing}: pytest (backend; dev dep \texttt{>=7.4.3}), Vitest (frontend; dev dep \texttt{1.3.0}), Playwright (E2E; \texttt{@playwright/test 1.45.2})
  \end{itemize}
\end{document}