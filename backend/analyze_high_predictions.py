#!/usr/bin/env python3
"""
Analyse: Welche Features verursachen hohe Vorhersagen (>95%)?
"""
import requests
import sys

# Sample-Patienten abrufen
try:
    patients = requests.get('http://localhost:8000/api/v1/patients/').json()
except Exception as e:
    print(f"‚ùå Fehler beim Abrufen der Patienten: {e}")
    print("   Ist der Backend-Container aktiv? (docker compose ps)")
    sys.exit(1)

print('='*80)
print('ANALYSE: Welche Features verursachen hohe Vorhersagen?')
print('='*80)

# Analysiere jeden Patienten
for patient in patients[:5]:
    pid = patient['id']
    name = patient.get('display_name', 'Unbekannt')
    
    # Hole Prediction
    pred_response = requests.get(f'http://localhost:8000/api/v1/patients/{pid}/predict').json()
    pred = pred_response['prediction']
    
    # Hole Explainer mit Feature Contributions
    expl_response = requests.get(f'http://localhost:8000/api/v1/patients/{pid}/explainer').json()
    feature_importance = expl_response['feature_importance']
    
    # Sortiere nach absolutem Beitrag
    sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)
    
    # Trenne positive und negative Features
    positive_features = [(f, v) for f, v in sorted_features if v > 0.1]
    negative_features = [(f, v) for f, v in sorted_features if v < -0.1]
    
    print(f'\n{"‚îÄ"*80}')
    print(f'Patient: {name} (ID: {pid})')
    print(f'Vorhersage: {pred:.1%}')
    print(f'{"‚îÄ"*80}')
    
    if positive_features:
        print(f'\n  ‚úÖ Positive Features (erh√∂hen Wahrscheinlichkeit):')
        for feat, val in positive_features[:8]:
            print(f'     {feat[:50]:52s} +{val:.4f}')
    else:
        print('  (keine stark positiven Features)')
    
    if negative_features:
        print(f'\n  ‚ö†Ô∏è  Negative Features (senken Wahrscheinlichkeit):')
        for feat, val in negative_features[:5]:
            print(f'     {feat[:50]:52s} {val:.4f}')

print('\n' + '='*80)
print('FAZIT: Medizinische Plausibilit√§t von hohen Vorhersagen')
print('='*80)

print('''
üìä MATHEMATISCHE BASIS:
   ‚úÖ Formel: P = 1 / (1 + e^(-(Intercept + Œ£(Koeffizient √ó Feature))))
   ‚úÖ Intercept: 1.2078 ‚Üí Basiswahrscheinlichkeit: 77.0%
   ‚úÖ F√ºr 99.9% (‚âà100%): Ben√∂tigt Logit ‚â• 7
      ‚Üí Erreicht durch Summe mehrerer positiver Features

üéØ TARGET VARIABLE:
   ‚Ä¢ outcome_measurments.post24.measure (Skala 0-55)
   ‚Ä¢ Je h√∂her, desto besser das H√∂rverm√∂gen nach 24 Monaten
   ‚Ä¢ Modell trainiert auf: post24 > Schwellwert = "Erfolg"

‚úÖ 100% BEDEUTET:
   ‚Ä¢ Patient hat "optimales Prognoseprofil"
   ‚Ä¢ Alle trainierten Features zeigen auf "bestes Outcome"
   ‚Ä¢ NICHT: Garantierter Erfolg, sondern h√∂chste Wahrscheinlichkeit

‚ö†Ô∏è  MEDIZINISCHE PLAUSIBILIT√ÑT PR√úFEN:
   1. Sind die aktivierten Features klinisch sinnvoll?
      ‚Üí z.B. CI im Gegenohr, gute H√∂ranamnese, j√ºngeres Alter
   
   2. Gibt es vergleichbare Patienten in Trainingsdaten?
      ‚Üí Muss mit Original-Datensatz abgeglichen werden
   
   3. Ist das Modell kalibriert?
      ‚Üí Calibration Plot: Predicted Probability vs. Actual Outcome
      ‚Üí Brier Score, Log-Loss zur Qualit√§tspr√ºfung

üîç WIE KORREKTHEIT VALIDIEREN:
   ‚úÖ Manuelle Berechnung = API Prediction
      ‚Üí Siehe: validate_predictions.py (bereits getestet)
   
   ‚úÖ Konsistenz zwischen Endpoints
      ‚Üí /predict und /explainer liefern identische Werte
   
   ‚ö†Ô∏è  Kalibrierung mit Testdaten:
      ‚Üí Wenn Modell sagt "100%", wie oft ist Outcome wirklich gut?
      ‚Üí Expected Calibration Error (ECE) berechnen
   
   ‚ö†Ô∏è  Cross-Validation:
      ‚Üí Wie gut generalisiert das Modell auf ungesehene Daten?

üí° EMPFEHLUNGEN F√úR UI:
   ‚Ä¢ Statt "100%" ‚Üí "Sehr hohe Erfolgswahrscheinlichkeit (>95%)"
   ‚Ä¢ Confidence Intervals anzeigen (falls verf√ºgbar)
   ‚Ä¢ Disclaimer: "Basierend auf historischen Daten von [N] Patienten"
   ‚Ä¢ Feature Contributions zeigen: "Diese Faktoren unterst√ºtzen die Prognose"
   ‚Ä¢ Bei 100%: "Bestes bekanntes Prognoseprofil - individuelle Ergebnisse k√∂nnen variieren"

üìù N√ÑCHSTE SCHRITTE ZUR VALIDIERUNG:
   1. Originaldaten anfordern und Kalibrierungskurve erstellen
   2. Feature-Verteilungen pr√ºfen: Gibt es Patienten mit √§hnlichem Profil?
   3. Mit medizinischem Fachpersonal abgleichen: Sind Prognosen realistisch?
   4. Test-Set Performance analysieren: Precision, Recall, ROC-AUC
''')
